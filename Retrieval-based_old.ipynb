{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building intelligent bots. Retrieval-based chatbots\n",
    "\n",
    "In this section we build a retrieval-based chatbot with Rasa. Before we go to this point, we go through a few NLP methods and word vectorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP methods for NLU\n",
    "\n",
    "Let's take one of President Trump's speech and divide into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "file = open(\"trump.txt\", \"r\",encoding='utf-8') \n",
    "trump = file.read() \n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(trump)\n",
    "\n",
    "for span in doc.sents:\n",
    "    print(\"> \", span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have are able to divide it using SpaCy and get the part of speech of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for span in doc.sents:\n",
    "    for i in range(span.start, span.end):\n",
    "        token = doc[i]\n",
    "        print(i, token.text, token.pos_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaller example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = \"Broadcasting today, live from KrakÃ³w, on chatbots.\"\n",
    "\n",
    "doc = nlp(sample)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks\n",
    "\n",
    "This NLP method is used to get the nouns from any sentene. It's important to understand what is the sentence about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(sample)\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "NER is a NLP method where we get not the nouns or part of speech, but meanings of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(sample)\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectorization\n",
    "\n",
    "Word vectorization is a process of preparing a vector representing each word. Gensim has an implementation of Word2Vec. We use a dimension of 100 and distance between two words in a sentence to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the vocabulary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "print(vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we just use the TSNE to reduce the dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw the words in a two-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n",
    "It is a simpler implementation of word2vec. It is faster as it takes only a few terms in each iteration for training insted of the whole dataset as in previous example. This is why it's called negative sampling.\n",
    "\n",
    "First of all, we define helper methods that are used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def randn(*dims):\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "def sigmoid(batch, stochastic=False):\n",
    "    return  1.0 / (1.0 + np.exp(-batch))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open(\"datasets/text8.dat.gz\", \"rb\") as f:\n",
    "    train_dict, train_set, train_tokens = pickle.load(f)\n",
    "\n",
    "train_set = np.random.permutation(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Config = namedtuple(\"Config\", [\"dict_size\", \"vect_size\", \"neg_samples\", \"updates\", \"learning_rate\", \n",
    "                               \"learning_rate_decay\", \"decay_period\", \"log_period\"])\n",
    "conf = Config(\n",
    "    dict_size=len(train_dict),\n",
    "    vect_size=100,\n",
    "    neg_samples=10,\n",
    "    updates=5000000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.995,\n",
    "    decay_period=10000,\n",
    "    log_period=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_sample(conf, train_set, train_tokens): # implemented\n",
    "    Vp = randn(conf.dict_size, conf.vect_size)\n",
    "    Vo = randn(conf.dict_size, conf.vect_size)\n",
    "\n",
    "    J = 0.0\n",
    "    learning_rate = conf.learning_rate\n",
    "    for i in range(conf.updates):\n",
    "        idx = i % len(train_set)\n",
    "\n",
    "        word    = train_set[idx, 0]\n",
    "        context = train_set[idx, 1]\n",
    "        \n",
    "        neg_context = np.random.randint(0, len(train_tokens), conf.neg_samples)\n",
    "        neg_context = train_tokens[neg_context]\n",
    "\n",
    "        word_vect = Vp[word, :]              # word vector\n",
    "        context_vect = Vo[context, :];       # context wector\n",
    "        negative_vects = Vo[neg_context, :]  # sampled negative vectors\n",
    "\n",
    "        # Cost and gradient calculation starts here\n",
    "        score_pos = word_vect @ context_vect.T\n",
    "        score_neg = word_vect @ negative_vects.T\n",
    "\n",
    "        J -= np.log(sigmoid(score_pos)) + np.sum(np.log(sigmoid(-score_neg)))\n",
    "        if (i + 1) % conf.log_period == 0:\n",
    "            print('Update {0}\\tcost: {1:>2.2f}'.format(i + 1, J / conf.log_period))\n",
    "            final_cost = J / conf.log_period\n",
    "            J = 0.0\n",
    "\n",
    "        pos_g = 1.0 - sigmoid(score_pos)\n",
    "        neg_g = sigmoid(score_neg)\n",
    "\n",
    "        word_grad = #fill \n",
    "        context_grad = # fill\n",
    "        neg_context_grad = # fill \n",
    "\n",
    "        Vp[word, :] -= learning_rate * word_grad\n",
    "        Vo[context, :] -= learning_rate * context_grad\n",
    "        Vo[neg_context, :] -= learning_rate * neg_context_grad\n",
    "\n",
    "        if i % conf.decay_period == 0:\n",
    "            learning_rate = learning_rate * conf.learning_rate_decay\n",
    "\n",
    "    return Vp, Vo, final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vp, Vo, J = neg_sample(conf, train_set, train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_word_idx(word, word_dict):\n",
    "    try:\n",
    "        return np.argwhere(np.array(word_dict) == word)[0][0]\n",
    "    except:\n",
    "        raise Exception(\"No such word in dict: {}\".format(word))\n",
    "\n",
    "def similar_words(embeddings, word, word_dict, hits):\n",
    "    word_idx = lookup_word_idx(word, word_dict)\n",
    "    similarity_scores = embeddings @ embeddings[word_idx]\n",
    "    similar_word_idxs = np.argsort(-similarity_scores)    \n",
    "    return [word_dict[i] for i in similar_word_idxs[:hits]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\nTraining cost: {0:>2.2f}\\n\\n'.format(J))\n",
    "\n",
    "sample_words = ['zero', 'computer', 'cars', 'home', 'album']\n",
    "\n",
    "Vp_norm = Vp / as_matrix(np.linalg.norm(Vp , axis=1))\n",
    "for w in sample_words:\n",
    "    similar = similar_words(Vp_norm, w, train_dict, 5)\n",
    "    print('Words similar to {}: {}'.format(w, \", \".join(similar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measure through vectors\n",
    "\n",
    "SpaCy already has words vectorized and we can simply check the similarity between two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc1 = nlp(u\"Warsaw is the largest city in Poland.\")\n",
    "doc2 = nlp(u\"Crossaint is baked in France.\")\n",
    "doc3 = nlp(u\"An emu is a large bird.\")\n",
    "\n",
    "for doc in [doc1, doc2, doc3]:\n",
    "    for other_doc in [doc1, doc2, doc3]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice example of word vectorization done by some researchers at Warsaw University: [Word2Vec](https://lamyiowce.github.io/word2viz/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-based chatbot\n",
    "\n",
    "In this section we use Rasa to build a very simple HR assistant bot. We can use Rasa as a server or use it directly from Python level. To start Rasa server you need to execute the following command:\n",
    "```python3 -m rasa_nlu.server --path projects```.\n",
    "It starts a server on default port 5000. You can test it using the request package. We should get the intent of the phrase `hi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_intent(sentence):\n",
    "    url = \"http://localhost:5000/parse\"\n",
    "    payload = {\"q\":sentence}\n",
    "    response = requests.get(url,params=payload)    \n",
    "    print(response.json())\n",
    "    intent = response.json()['intent']\n",
    "    if intent['confidence'] > 0.5: \n",
    "        return intent['name']\n",
    "    return response.json()\n",
    "\n",
    "get_intent(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To use Rasa from Python level you need to prepare a config file that contains the pipeline and the filename of examples used for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "{\n",
    "  \"pipeline\": \"spacy_sklearn\",\n",
    "  \"path\" : \".\",\n",
    "  \"data\" : \".anna.json\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config_file = open(\"config.json\", \"w\")\n",
    "config_file.write(config)\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data file contains examples that are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anna_common_examples = \"\"\"\n",
    "{\n",
    "  \"rasa_nlu_data\": {\n",
    "    \"entity_synonyms\": [\n",
    "      {\n",
    "        \"value\": \"candidate\",\n",
    "        \"synonyms\": [\"developer\", \"data scientist\"]\n",
    "      }\n",
    "    ],\n",
    "    \"common_examples\": [\n",
    "      {\n",
    "        \"text\": \"hey\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"howdy\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hey there\",\n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hello\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hi\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good morning\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good evening\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"dear sir\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"yes\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yep\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yeah\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"indeed\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"that's right\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"ok\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"right, thank you\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "      \"start\": 5,\n",
    "      \"end\": 13,\n",
    "      \"value\": \"candidate\",\n",
    "      \"entity\": \"candidate\"\n",
    "        }\n",
    "        ]\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"adding candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "              \"start\": 8,\n",
    "              \"end\": 16,\n",
    "              \"value\": \"candidate\",\n",
    "              \"entity\": \"candidate\"\n",
    "            }        \n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"please add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },              \n",
    "      {\n",
    "        \"text\": \"please add new candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },           \n",
    "      {\n",
    "        \"text\": \"we have new prescreening upcoming\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"we have a new candidate for prescreening\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"correct\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great choice\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"sounds really good\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"goodbye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"good bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"stop\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"end\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"farewell\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"Bye bye\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"have a good one\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "training_data = open(\"anna.json\", \"w\")\n",
    "training_data.write(anna_common_examples)\n",
    "training_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu.model import Trainer, Interpreter\n",
    "from rasa_nlu.components import ComponentBuilder\n",
    "import rasa_nlu.config\n",
    "\n",
    "cfg = 'config.json'\n",
    "training_data = load_data('anna.json')\n",
    "trainer = Trainer(rasa_nlu.config.load(cfg))\n",
    "trainer.train(training_data)\n",
    "model_directory = trainer.persist('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the intent we use the parse method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.model import Metadata, Interpreter\n",
    "\n",
    "interpreter = Interpreter.load(model_directory)\n",
    "\n",
    "interpreter.parse(u\"add developer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2\n",
    "\n",
    "Extend the training examples and add an intent `change_status` with entities: `passed` and `failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anna_common_examples = \"\"\"\n",
    "{\n",
    "  \"rasa_nlu_data\": {\n",
    "    \"entity_synonyms\": [\n",
    "      {\n",
    "        \"value\": \"candidate\",\n",
    "        \"synonyms\": [\"developer\", \"data scientist\"]\n",
    "      }\n",
    "    ],\n",
    "    \"common_examples\": [\n",
    "      {\n",
    "        \"text\": \"hey\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"howdy\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hey there\",\n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hello\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hi\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good morning\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good evening\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"dear sir\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"yes\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yep\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yeah\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"indeed\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"that's right\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"ok\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"right, thank you\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "      \"start\": 5,\n",
    "      \"end\": 13,\n",
    "      \"value\": \"candidate\",\n",
    "      \"entity\": \"candidate\"\n",
    "        }\n",
    "        ]\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"adding candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "              \"start\": 8,\n",
    "      \"end\": 16,\n",
    "      \"value\": \"candidate\",\n",
    "      \"entity\": \"candidate\"\n",
    "        }        \n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"please add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },              \n",
    "      {\n",
    "        \"text\": \"please add new candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },           \n",
    "      {\n",
    "        \"text\": \"we have new prescreening upcoming\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"we have a new candidate for prescreening\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"correct\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great choice\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"sounds really good\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"goodbye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"good bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"stop\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"end\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"farewell\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"Bye bye\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"have a good one\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "training_data = open(\"anna_new.json\", \"w\")\n",
    "training_data.write(anna_common_examples)\n",
    "training_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.converters import load_data\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "cfg = 'config.json'\n",
    "training_data = load_data('anna_new.json')\n",
    "trainer = Trainer(rasa_nlu.config.load(cfg))\n",
    "trainer.train(training_data)\n",
    "model_directory = trainer.persist('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.model import Metadata, Interpreter\n",
    "\n",
    "interpreter = Interpreter.load(model_directory)\n",
    "\n",
    "interpreter.parse(u\"the developer didn't passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building stories with Rasa\n",
    "\n",
    "To build a chatbot with Rasa that has a focus on the context management, we need to build the dataset of intents as before, but also stories. \n",
    "\n",
    "Rasa core for building stories need a bit more configuration than in the previous example. We need to setup the following:\n",
    "- the configuration of language and machine learning backend,\n",
    "- setup the domain with sample chatbot responses,\n",
    "- define the stories.\n",
    "After this step we need to train Rasa, but we still need feed it with intents after it.\n",
    "\n",
    "A basic configuration for Rasa is needed like to language and pipeline. The pipeline defines the way how we want to train our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rasa_config = \"\"\"\n",
    "policies:\n",
    "  - name: KerasPolicy\n",
    "    epochs: 100\n",
    "    max_history: 5\n",
    "  - name: FallbackPolicy\n",
    "    fallback_action_name: 'action_default_fallback'\n",
    "  - name: MemoizationPolicy\n",
    "    max_history: 5\n",
    "  - name: FormPolicy\n",
    "\"\"\"\n",
    "%store rasa_config > rasa_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a few stories for our chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories_md = \"\"\"\n",
    "## happy path\n",
    "* greet\n",
    "  - utter_greet\n",
    "* mood_great\n",
    "  - utter_happy\n",
    "\n",
    "## sad path 1\n",
    "* greet\n",
    "  - utter_greet\n",
    "* mood_unhappy\n",
    "  - utter_cheer_up\n",
    "  - utter_did_that_help\n",
    "* mood_affirm\n",
    "  - utter_happy\n",
    "\n",
    "## sad path 2\n",
    "* greet\n",
    "  - utter_greet\n",
    "* mood_unhappy\n",
    "  - utter_cheer_up\n",
    "  - utter_did_that_help\n",
    "* mood_deny\n",
    "  - utter_goodbye\n",
    "\n",
    "## say goodbye\n",
    "* goodbye\n",
    "  - utter_goodbye\n",
    "\"\"\"\n",
    "%store stories_md > stories.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain_yml = \"\"\"\n",
    "intents:\n",
    "  - greet\n",
    "  - goodbye\n",
    "  - mood_affirm\n",
    "  - mood_deny\n",
    "  - mood_great\n",
    "  - mood_unhappy\n",
    "\n",
    "actions:\n",
    "- utter_greet\n",
    "- utter_cheer_up\n",
    "- utter_did_that_help\n",
    "- utter_happy\n",
    "- utter_goodbye\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "  - text: \"Hey! How are you?\"\n",
    "\n",
    "  utter_cheer_up:\n",
    "  - text: \"Here is something to cheer you up:\"\n",
    "    image: \"https://i.imgur.com/nGF1K8f.jpg\"\n",
    "\n",
    "  utter_did_that_help:\n",
    "  - text: \"Did that help you?\"\n",
    "\n",
    "  utter_happy:\n",
    "  - text: \"Great carry on!\"\n",
    "\n",
    "  utter_goodbye:\n",
    "  - text: \"Bye\"\n",
    "\"\"\"\n",
    "%store domain_yml > domain.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the stories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python3 -m rasa_core.train -d domain.yml -s stories.md -o models/oreilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the intents like in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlu_md = \"\"\"\n",
    "## intent:greet\n",
    "- hey\n",
    "- hello\n",
    "- hi\n",
    "- good morning\n",
    "- good evening\n",
    "- hey there\n",
    "\n",
    "## intent:goodbye\n",
    "- bye\n",
    "- goodbye\n",
    "- see you around\n",
    "- see you later\n",
    "\n",
    "## intent:mood_affirm\n",
    "- yes\n",
    "- indeed\n",
    "- of course\n",
    "- that sounds good\n",
    "- correct\n",
    "\n",
    "## intent:mood_deny\n",
    "- no\n",
    "- never\n",
    "- I don't think so\n",
    "- don't like that\n",
    "- no way\n",
    "- not really\n",
    "\n",
    "## intent:mood_great\n",
    "- perfect\n",
    "- very good\n",
    "- great\n",
    "- amazing\n",
    "- wonderful\n",
    "- I am feeling very good\n",
    "- I am great\n",
    "- I'm good\n",
    "\n",
    "## intent:mood_unhappy\n",
    "- sad\n",
    "- very sad\n",
    "- unhappy\n",
    "- bad\n",
    "- very bad\n",
    "- awful\n",
    "- terrible\n",
    "- not very good\n",
    "- extremely sad\n",
    "- so sad\n",
    "\"\"\"\n",
    "%store nlu_md > nlu.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLU configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlu_config = \"\"\"\n",
    "language: en\n",
    "pipeline: tensorflow_embedding\n",
    "\"\"\"\n",
    "%store nlu_config > nlu_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa NLU training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python3 -m rasa_nlu.train -c nlu_config.yml --data nlu.md -o models --fixed_model_name nlu --project oreilly --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import clear_output, HTML, display\n",
    "from rasa_core.agent import Agent\n",
    "from rasa_core.interpreter import RasaNLUInterpreter\n",
    "import time\n",
    "\n",
    "interpreter = RasaNLUInterpreter('models/oreilly/nlu')\n",
    "messages = [\"Hi! you can chat in this window. Type 'stop' to end the conversation.\"]\n",
    "agent = Agent.load('models/oreilly', interpreter=interpreter)\n",
    "\n",
    "def chatlogs_html(messages):\n",
    "    messages_html = \"\".join([\"<p>{}</p>\".format(m) for m in messages])\n",
    "    chatbot_html = \"\"\"<div class=\"chat-window\" {}</div>\"\"\".format(messages_html)\n",
    "    return chatbot_html\n",
    "\n",
    "\n",
    "while True:\n",
    "    clear_output()\n",
    "    display(HTML(chatlogs_html(messages)))\n",
    "    time.sleep(0.3)\n",
    "    a = input()\n",
    "    messages.append(a)\n",
    "    if a == 'stop':\n",
    "        break\n",
    "    responses = agent.handle_message(a)\n",
    "    for r in responses:\n",
    "        messages.append(r.get(\"text\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
